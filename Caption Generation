{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":3033392,"datasetId":1857779,"databundleVersionId":3081441},{"sourceType":"datasetVersion","sourceId":1111676,"datasetId":623289,"databundleVersionId":1141936},{"sourceType":"datasetVersion","sourceId":789959,"datasetId":412733,"databundleVersionId":811637}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Import libraries\n","metadata":{}},{"cell_type":"code","source":"import torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:44:23.693328Z","iopub.execute_input":"2026-02-25T04:44:23.693547Z","iopub.status.idle":"2026-02-25T04:44:27.293239Z","shell.execute_reply.started":"2026-02-25T04:44:23.693526Z","shell.execute_reply":"2026-02-25T04:44:27.292472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport os\nimport pickle\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchmetrics.text import ROUGEScore\nfrom torchmetrics.text import BLEUScore\nfrom torchvision import transforms\nfrom torchvision.models import convnext_small\nfrom torchvision.transforms import Normalize, ToPILImage, ToTensor\nfrom transformers import GPT2Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:43.228794Z","iopub.execute_input":"2026-02-25T04:45:43.229663Z","iopub.status.idle":"2026-02-25T04:45:43.234892Z","shell.execute_reply.started":"2026-02-25T04:45:43.229631Z","shell.execute_reply":"2026-02-25T04:45:43.234155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2.Set Random Seed\n","metadata":{}},{"cell_type":"code","source":"# Set a seed value for reproducibility\nseed = 42\n# Set the seed for PyTorch\ntorch.manual_seed(seed)\n# Set the seed for NumPy to maintain consistency in random operations\nnp.random.seed(seed)\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:47.169221Z","iopub.execute_input":"2026-02-25T04:45:47.169572Z","iopub.status.idle":"2026-02-25T04:45:47.175162Z","shell.execute_reply.started":"2026-02-25T04:45:47.169530Z","shell.execute_reply":"2026-02-25T04:45:47.174522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# explore datasets\nimg_dir = \"/kaggle/input/datasets/adityajn105/flickr8k/Images\"\ntext_dir = \"/kaggle/input/datasets/adityajn105/flickr8k\"\n\n# load the captions for the given image\ncaptions = pd.read_csv(Path(text_dir, \"captions.txt\"))\n\n# The Flickr8k dataset from adityajn105 has a different structure\n# It has columns: image, caption\n# Let's check the actual structure first\nprint(captions.head())\nprint(f\"Columns: {captions.columns.tolist()}\")\n\nimages = os.listdir(img_dir)\nindex = np.random.randint(0, len(images))\nselected_file = images[index]\n\n# Adjust based on the actual column names\nselected_captions = captions[captions[\"image\"] == selected_file][\"caption\"].tolist()\n\nfor idx, caption in enumerate(selected_captions):\n    print(f\"Caption {idx}: {caption}\")\n    \nImage.open(Path(img_dir, selected_file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:48.794503Z","iopub.execute_input":"2026-02-25T04:45:48.795126Z","iopub.status.idle":"2026-02-25T04:45:49.081647Z","shell.execute_reply.started":"2026-02-25T04:45:48.795099Z","shell.execute_reply":"2026-02-25T04:45:49.080841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.Load and Visualize the Data","metadata":{}},{"cell_type":"markdown","source":"# 4.Create a Transformation Function","metadata":{}},{"cell_type":"code","source":"class ResizePadTransform:\n    def __init__(self, target_size):\n        self.target_size = target_size\n        \n    def __call__(self, image):\n        # Calculate the aspect ratio of the original image\n        width, height = image.size\n        aspect_ratio = width / height\n\n        # Determine the size after resizing while preserving the aspect ratio\n        if width > height:\n            new_width = self.target_size\n            new_height = int(self.target_size / aspect_ratio)\n        else:\n            new_height = self.target_size\n            new_width = int(self.target_size * aspect_ratio)\n\n        # Define a torchvision transform to resize the image\n        resize_transform = transforms.Resize((new_height, new_width))\n\n        # Resize the image using the defined transform\n        resized_image = resize_transform(image)\n\n        # Calculate the padding required to achieve the target size\n        pad_width = self.target_size - new_width\n        pad_height = self.target_size - new_height\n        \n        # If pad is odd, then it will have issues, so you need to fix it\n        pad_left = pad_width // 2\n        pad_right = pad_width - pad_left\n\n        pad_top = pad_height // 2\n        pad_bottom = pad_height - pad_top\n\n        # Create a padding transform using torchvision\n        padding_transform = transforms.Pad((pad_left, pad_top, pad_right, pad_bottom))\n\n        # Apply the padding transform to the resized image\n        padded_resized_image = padding_transform(resized_image)\n\n        return padded_resized_image\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntfms = transforms.Compose([\n    ResizePadTransform(224),\n    ToTensor(),\n    transforms.Normalize(mean=mean, std=std),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:52.312655Z","iopub.execute_input":"2026-02-25T04:45:52.313286Z","iopub.status.idle":"2026-02-25T04:45:52.319591Z","shell.execute_reply.started":"2026-02-25T04:45:52.313259Z","shell.execute_reply":"2026-02-25T04:45:52.318894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a Tokenizer","metadata":{}},{"cell_type":"code","source":"import json\nfrom collections import Counter\n\nclass SimpleTokenizer:\n    def __init__(self):\n        self.word2idx = {\"<|startoftext|>\": 0, \"<|endoftext|>\": 1, \"[PAD]\": 2, \"<|unk|>\": 3}\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.vocab_size = len(self.word2idx)\n        \n    def build_vocab(self, captions, max_vocab_size=10000):\n        \"\"\"Build vocabulary from captions\"\"\"\n        word_counts = Counter()\n        for caption in captions:\n            words = caption.lower().split()\n            word_counts.update(words)\n        \n        # Add most common words to vocabulary\n        for word, _ in word_counts.most_common(max_vocab_size - len(self.word2idx)):\n            if word not in self.word2idx:\n                self.word2idx[word] = len(self.word2idx)\n                self.idx2word[len(self.idx2word)] = word\n        \n        self.vocab_size = len(self.word2idx)\n        print(f\"Vocabulary size: {self.vocab_size}\")\n    \n    def encode(self, text):\n        \"\"\"Convert text to token IDs\"\"\"\n        words = text.lower().split()\n        return [self.word2idx.get(word, self.word2idx[\"<|unk|>\"]) for word in words]\n    \n    def decode(self, token_ids):\n        \"\"\"Convert token IDs back to text\"\"\"\n        return \" \".join([self.idx2word.get(idx, \"<|unk|>\") for idx in token_ids])\n    \n    def __len__(self):\n        return self.vocab_size\n\n# Build tokenizer from your captions\ntokenizer = SimpleTokenizer()\ntokenizer.build_vocab(captions[\"caption\"].tolist())\n\n# Test it\nprint(tokenizer.encode(\"A cat is playing.\"))\nprint(tokenizer.decode(tokenizer.encode(\"This is a cat playing.\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:55.304157Z","iopub.execute_input":"2026-02-25T04:45:55.304845Z","iopub.status.idle":"2026-02-25T04:45:55.408549Z","shell.execute_reply.started":"2026-02-25T04:45:55.304804Z","shell.execute_reply":"2026-02-25T04:45:55.407979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transform the Dataset","metadata":{}},{"cell_type":"code","source":"# Custom dataset class for handling image-caption pairs\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, data_split, transform=None, tokenizer=None, max_len=50, phase=\"train\"):\n        # Define paths for images and captions\n        self.img_folder = Path(root_dir, \"Images\")\n        \n        # Load all captions\n        self.all_captions = pd.read_csv(Path(root_dir, \"captions.txt\"))\n        \n        # Create train/val/test split (80/10/10)\n        unique_images = self.all_captions['image'].unique()\n        np.random.seed(42)\n        np.random.shuffle(unique_images)\n        \n        n_train = int(0.8 * len(unique_images))\n        n_val = int(0.1 * len(unique_images))\n        \n        if data_split == \"train\":\n            self.images = unique_images[:n_train]\n        elif data_split == \"dev\":\n            self.images = unique_images[n_train:n_train+n_val]\n        else:  # test\n            self.images = unique_images[n_train+n_val:]\n        \n        # Filter captions for this split\n        self.captions = self.all_captions[self.all_captions['image'].isin(self.images)]\n        \n        # Store transformations, tokenizer, max caption length, and dataset phase\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.phase = phase\n    \n    # Return the total number of captions in the dataset\n    def __len__(self):\n        if self.phase == \"train\":\n            return len(self.captions)\n        else:\n            return len(self.images)\n    \n    # Retrieve an image-caption pair\n    def __getitem__(self, idx):\n        if self.phase == \"train\":\n            caption_row = self.captions.iloc[idx]\n            caption = caption_row[\"caption\"]\n            caption = f\"<|startoftext|> {caption} <|endoftext|>\"\n            caption = self.tokenizer.encode(caption)\n            pad_tokens = self.max_len - len(caption) + 1\n            caption += pad_tokens * [self.tokenizer.word2idx[\"[PAD]\"]]\n            caption = torch.tensor(caption)\n            image_name = caption_row[\"image\"]\n            image = Image.open(Path(self.img_folder, image_name))\n            image = self.transform(image)\n        else:\n            image_name = self.images[idx]\n            image = Image.open(Path(self.img_folder, image_name))\n            image = self.transform(image)\n            caption = self.captions[self.captions[\"image\"] == image_name][\"caption\"].tolist()\n        \n        return {\"image\": image, \"caption\": caption, \"image_name\": image_name}\n\n# Create dataset instances for training, evaluation, and testing\ntrain_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"train\", \n    transform=tfms, \n    tokenizer=tokenizer\n)\neval_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"dev\", \n    transform=tfms, \n    tokenizer=tokenizer\n)\ntest_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"test\", \n    transform=tfms, \n    tokenizer=tokenizer, \n    phase=\"test\"\n)\n\n# Print the number of samples in each dataset split\nprint(f\"Train: {len(train_dataset)}, Val: {len(eval_dataset)}, Test: {len(test_dataset)}\")\n\n# Print the tokenized caption of the 100th training sample\nprint(train_dataset[100][\"caption\"])\n\n# Decode and print the caption in human-readable format\nprint(tokenizer.decode(train_dataset[100][\"caption\"].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:45:57.754984Z","iopub.execute_input":"2026-02-25T04:45:57.755593Z","iopub.status.idle":"2026-02-25T04:45:57.983069Z","shell.execute_reply.started":"2026-02-25T04:45:57.755559Z","shell.execute_reply":"2026-02-25T04:45:57.982315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom dataset class for handling image-caption pairs\nclass CustomImageDataset(Dataset):\n    def __init__(self, root_dir, data_split, transform=None, tokenizer=None, max_len=50, phase=\"train\"):\n        # Define paths for images and captions\n        self.img_folder = Path(root_dir, \"Images\")\n        \n        # Load all captions\n        self.all_captions = pd.read_csv(Path(root_dir, \"captions.txt\"))\n        \n        # Create train/val/test split (80/10/10)\n        unique_images = self.all_captions['image'].unique()\n        np.random.seed(42)\n        np.random.shuffle(unique_images)\n        \n        n_train = int(0.8 * len(unique_images))\n        n_val = int(0.1 * len(unique_images))\n        \n        if data_split == \"train\":\n            self.images = unique_images[:n_train]\n        elif data_split == \"dev\":\n            self.images = unique_images[n_train:n_train+n_val]\n        else:  # test\n            self.images = unique_images[n_train+n_val:]\n        \n        # Filter captions for this split\n        self.captions = self.all_captions[self.all_captions['image'].isin(self.images)]\n        \n        # Store transformations, tokenizer, max caption length, and dataset phase\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.phase = phase\n    \n    # Return the total number of captions in the dataset\n    def __len__(self):\n        if self.phase == \"train\":\n            return len(self.captions)\n        else:\n            return len(self.images)\n    \n    # Retrieve an image-caption pair\n    def __getitem__(self, idx):\n        if self.phase == \"train\":\n            caption_row = self.captions.iloc[idx]\n            caption = caption_row[\"caption\"]\n            caption = f\"<|startoftext|> {caption} <|endoftext|>\"\n            caption = self.tokenizer.encode(caption)\n            \n            pad_tokens = self.max_len - len(caption) + 1\n            caption += pad_tokens * [self.tokenizer.word2idx[\"[PAD]\"]]\n            caption = torch.tensor(caption)\n            \n            image_name = caption_row[\"image\"]\n            image = Image.open(Path(self.img_folder, image_name))\n            image = self.transform(image)\n        else:\n            image_name = self.images[idx]\n            image = Image.open(Path(self.img_folder, image_name))\n            image = self.transform(image)\n            caption = self.captions[self.captions[\"image\"] == image_name][\"caption\"].tolist()\n        \n        return {\"image\": image, \"caption\": caption, \"image_name\": image_name}\n\ntrain_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"train\", \n    transform=tfms, \n    tokenizer=tokenizer,\n    phase=\"train\"  # Always use train phase for proper tensor output\n)\n\neval_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"dev\", \n    transform=tfms, \n    tokenizer=tokenizer,\n    phase=\"train\"  # Changed from \"eval\" to \"train\" so it returns tensors\n)\n\ntest_dataset = CustomImageDataset(\n    root_dir=\"/kaggle/input/datasets/adityajn105/flickr8k\", \n    data_split=\"test\", \n    transform=tfms, \n    tokenizer=tokenizer,\n    phase=\"test\"  # Keep as \"test\" for inference\n)\n\n# Recreate the dataloaders with the fixed datasets\nBATCH_SIZE = 8\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\neval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"Dataloaders recreated:\")\nprint(f\"  Train batches: {len(train_dataloader)}\")\nprint(f\"  Eval batches: {len(eval_dataloader)}\")\nprint(f\"  Test batches: {len(test_dataloader)}\")\n# Print the tokenized caption of the 100th training sample\nprint(\"\\nSample caption (tokenized):\")\nprint(train_dataset[100][\"caption\"])\n\n# Decode and print the caption in human-readable format\nprint(\"\\nSample caption (decoded):\")\nprint(tokenizer.decode(train_dataset[100][\"caption\"].tolist()))\n\n\n# Define batch size for loading the dataset\nBATCH_SIZE = 64\n\n# Create DataLoader for the training dataset\n# - shuffle=True ensures that the dataset is shuffled at each epoch to improve training performance\n# - batch_size determines how many samples are processed in a single forward/backward pass\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n\n# Create DataLoader for the evaluation dataset\neval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\n\n# Create DataLoader for the test dataset\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# Print the number of batches in each DataLoader\nprint(f\"Train batches: {len(train_dataloader)} (was 4045, now ~506)\")\nprint(f\"Eval batches: {len(eval_dataloader)}\")\nprint(f\"Test batches: {len(test_dataloader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:02.767091Z","iopub.execute_input":"2026-02-25T04:46:02.767392Z","iopub.status.idle":"2026-02-25T04:46:02.959630Z","shell.execute_reply.started":"2026-02-25T04:46:02.767366Z","shell.execute_reply":"2026-02-25T04:46:02.958965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a DataLoader","metadata":{}},{"cell_type":"markdown","source":"# Prepare ConvNeXt Model","metadata":{}},{"cell_type":"code","source":"import os\n\n# Check what's in the ConvNeXt dataset\nconvnext_path = \"/kaggle/input/datasets/mithilsalunkhe/convnext\"\nprint(\"Contents of ConvNeXt dataset:\")\nfor item in os.listdir(convnext_path):\n    print(f\"  {item}\")\n\n# Load ConvNeXt model without downloading (weights=None first)\nconvnext = convnext_small(weights=None)\n\n# Load the pre-trained weights from the local dataset\n# The file is likely named something like \"convnext_small.pth\" or similar\nweight_files = os.listdir(convnext_path)\nprint(f\"\\nAvailable weight files: {weight_files}\")\n\n# Find the convnext_small weights file\nconvnext_weight_file = None\nfor f in weight_files:\n    if \"small\" in f.lower() and (f.endswith('.pth') or f.endswith('.pt')):\n        convnext_weight_file = f\n        break\n\nif convnext_weight_file:\n    weight_path = os.path.join(convnext_path, convnext_weight_file)\n    print(f\"Loading weights from: {weight_path}\")\n    \n    # Load the state dict\n    state_dict = torch.load(weight_path, map_location='cpu')\n    \n    # Handle different state dict formats\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n    elif 'state_dict' in state_dict:\n        state_dict = state_dict['state_dict']\n    \n    convnext.load_state_dict(state_dict, strict=False)\n    print(\"✓ Pre-trained weights loaded successfully!\")\nelse:\n    print(\"⚠ No pre-trained weights found, using random initialization\")\n\n# Remove the classification head (last layer) to use it as a feature extractor\nconvnext.classifier[2] = nn.Identity()\n\n# Freeze the ConvNeXt parameters so they don't update during training\nfor param in convnext.parameters():\n    param.requires_grad = False\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconvnext = convnext.to(device)\n\nprint(f\"\\nConvNeXt model prepared and moved to {device}\")\nprint(f\"ConvNeXt is frozen: {not next(convnext.parameters()).requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:09.653307Z","iopub.execute_input":"2026-02-25T04:46:09.654074Z","iopub.status.idle":"2026-02-25T04:46:10.583103Z","shell.execute_reply.started":"2026-02-25T04:46:09.654043Z","shell.execute_reply":"2026-02-25T04:46:10.582530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a Model using ConvNeXt and Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        convnext_weights_path=None,\n        d_model=768,\n        nhead=8,\n        num_decoder_layers=6,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        max_len=50\n    ):\n        super().__init__()\n        \n        # ConvNeXt feature extractor\n        self.convnext = convnext_small(weights=None)\n        \n        # Load pre-trained weights if available\n        if convnext_weights_path and os.path.exists(convnext_weights_path):\n            state_dict = torch.load(convnext_weights_path, map_location='cpu')\n            if 'model' in state_dict:\n                state_dict = state_dict['model']\n            elif 'state_dict' in state_dict:\n                state_dict = state_dict['state_dict']\n            self.convnext.load_state_dict(state_dict, strict=False)\n        \n        self.convnext.classifier[2] = nn.Identity()\n        \n        # Freeze ConvNeXt\n        for param in self.convnext.parameters():\n            param.requires_grad = False\n        \n        # Project ConvNeXt features to d_model dimensions\n        self.feature_projection = nn.Linear(768, d_model)\n        \n        # Token embedding layer\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n        \n        # Transformer decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            activation=activation,\n            batch_first=True\n        )\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer,\n            num_layers=num_decoder_layers\n        )\n        \n        # Output projection to vocabulary\n        self.fc_out = nn.Linear(d_model, vocab_size)\n        \n        self.d_model = d_model\n        \n    def forward(self, images, captions, tgt_mask=None, tgt_key_padding_mask=None):\n        # Extract features from images using ConvNeXt\n        image_features = self.convnext(images)  # [batch_size, 768]\n        \n        # Project to d_model dimensions\n        image_features = self.feature_projection(image_features)\n        image_features = image_features.unsqueeze(1)\n        \n        # Embed captions\n        caption_embeddings = self.embedding(captions) * math.sqrt(self.d_model)\n        caption_embeddings = caption_embeddings.transpose(0, 1)\n        caption_embeddings = self.pos_encoder(caption_embeddings)\n        caption_embeddings = caption_embeddings.transpose(0, 1)\n        \n        # Decode\n        output = self.transformer_decoder(\n            tgt=caption_embeddings,\n            memory=image_features,\n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n        \n        output = self.fc_out(output)\n        return output\n\n\n# Find the weight file\nconvnext_weight_file = None\nfor f in os.listdir(convnext_path):\n    if \"small\" in f.lower() and (f.endswith('.pth') or f.endswith('.pt')):\n        convnext_weight_file = os.path.join(convnext_path, f)\n        break\n\n# Initialize model with pre-trained weights\nvocab_size = len(tokenizer)\nmodel = ImageCaptioningModel(\n    vocab_size=vocab_size,\n    convnext_weights_path=convnext_weight_file,\n    d_model=768,\n    nhead=8,\n    num_decoder_layers=6,\n    dim_feedforward=2048,\n    dropout=0.1,\n    max_len=50\n)\n\nmodel = model.to(device)\nprint(f\"\\nModel created with vocabulary size: {vocab_size}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:14.633519Z","iopub.execute_input":"2026-02-25T04:46:14.634224Z","iopub.status.idle":"2026-02-25T04:46:15.745346Z","shell.execute_reply.started":"2026-02-25T04:46:14.634194Z","shell.execute_reply":"2026-02-25T04:46:15.744643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set the Loss function\n","metadata":{}},{"cell_type":"code","source":"pad_idx = tokenizer.word2idx[\"[PAD]\"]\ncriterion = CrossEntropyLoss(ignore_index=pad_idx)\nprint(f\"\\nLoss function set: CrossEntropyLoss with ignore_index={pad_idx}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:19.308597Z","iopub.execute_input":"2026-02-25T04:46:19.309369Z","iopub.status.idle":"2026-02-25T04:46:19.313434Z","shell.execute_reply.started":"2026-02-25T04:46:19.309337Z","shell.execute_reply":"2026-02-25T04:46:19.312783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set the Optimizer","metadata":{}},{"cell_type":"code","source":"# 2. Reduce epochs and set up training\nnum_epochs = 3  # 3 epochs is enough\nlearning_rate = 0.0001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\npad_idx = tokenizer.word2idx[\"[PAD]\"]\ncriterion = CrossEntropyLoss(ignore_index=pad_idx)\n\nbest_val_loss = float('inf')\ntrain_losses = []\nval_losses = []\n\nprint(\"Ready to train!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:22.587240Z","iopub.execute_input":"2026-02-25T04:46:22.587941Z","iopub.status.idle":"2026-02-25T04:46:22.594599Z","shell.execute_reply.started":"2026-02-25T04:46:22.587910Z","shell.execute_reply":"2026-02-25T04:46:22.593886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\nbest_val_loss = float('inf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:24.295127Z","iopub.execute_input":"2026-02-25T04:46:24.295421Z","iopub.status.idle":"2026-02-25T04:46:24.299302Z","shell.execute_reply.started":"2026-02-25T04:46:24.295398Z","shell.execute_reply":"2026-02-25T04:46:24.298544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_mask(src, tgt, pad_idx):\n    tgt_seq_len = tgt.shape[1]\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device=tgt.device)\n    tgt_padding_mask = (tgt == pad_idx)\n    return tgt_mask, tgt_padding_mask\n\ndef generate_square_subsequent_mask(sz, device=\"cpu\"):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\ndef train_epoch(model, dataloader, criterion, optimizer, device, pad_idx):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(dataloader):\n        images = batch[\"image\"].to(device)\n        captions = batch[\"caption\"].to(device)\n        caption_input = captions[:, :-1]\n        caption_target = captions[:, 1:]\n        tgt_mask, tgt_padding_mask = create_mask(images, caption_input, pad_idx)\n        optimizer.zero_grad()\n        output = model(images, caption_input, tgt_mask, tgt_padding_mask)\n        output = output.reshape(-1, output.shape[-1])\n        caption_target = caption_target.reshape(-1)\n        loss = criterion(output, caption_target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n        if (batch_idx + 1) % 100 == 0:\n            print(f\"  Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion, device, pad_idx):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch[\"image\"].to(device)\n            captions = batch[\"caption\"]\n            if isinstance(captions, list):\n                continue\n            captions = captions.to(device)\n            caption_input = captions[:, :-1]\n            caption_target = captions[:, 1:]\n            tgt_mask, tgt_padding_mask = create_mask(images, caption_input, pad_idx)\n            output = model(images, caption_input, tgt_mask, tgt_padding_mask)\n            output = output.reshape(-1, output.shape[-1])\n            caption_target = caption_target.reshape(-1)\n            loss = criterion(output, caption_target)\n            total_loss += loss.item()\n            num_batches += 1\n    return total_loss / num_batches if num_batches > 0 else float('inf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:25.395706Z","iopub.execute_input":"2026-02-25T04:46:25.396399Z","iopub.status.idle":"2026-02-25T04:46:25.406759Z","shell.execute_reply.started":"2026-02-25T04:46:25.396369Z","shell.execute_reply":"2026-02-25T04:46:25.406053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This will create download links after training\ndef save_and_download():\n    from IPython.display import FileLink, display\n    print(\"\\n\" + \"=\"*70)\n    print(\"DOWNLOAD THESE FILES NOW:\")\n    print(\"=\"*70)\n    display(FileLink('/kaggle/working/best_caption_model.pth'))\n    display(FileLink('/kaggle/working/full_checkpoint.pth'))\n    print(\"\\nRight-click each link above and 'Save Link As' to your laptop!\")\n    print(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:46:41.297144Z","iopub.execute_input":"2026-02-25T04:46:41.297976Z","iopub.status.idle":"2026-02-25T04:46:41.303874Z","shell.execute_reply.started":"2026-02-25T04:46:41.297937Z","shell.execute_reply":"2026-02-25T04:46:41.302995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"def create_mask(src, tgt, pad_idx):\n    tgt_seq_len = tgt.shape[1]\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device=tgt.device)\n    tgt_padding_mask = (tgt == pad_idx)\n    return tgt_mask, tgt_padding_mask\n\n\ndef generate_square_subsequent_mask(sz, device=\"cpu\"):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\n# Training function\ndef train_epoch(model, dataloader, criterion, optimizer, device, pad_idx):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(dataloader):\n        images = batch[\"image\"].to(device)\n        captions = batch[\"caption\"].to(device)\n        \n        # Create input and target sequences\n        # Input: all tokens except the last one\n        # Target: all tokens except the first one (shifted by 1)\n        caption_input = captions[:, :-1]\n        caption_target = captions[:, 1:]\n        \n        # Create masks\n        tgt_mask, tgt_padding_mask = create_mask(images, caption_input, pad_idx)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        output = model(images, caption_input, tgt_mask, tgt_padding_mask)\n        \n        # Calculate loss\n        output = output.reshape(-1, output.shape[-1])\n        caption_target = caption_target.reshape(-1)\n        loss = criterion(output, caption_target)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if (batch_idx + 1) % 100 == 0:\n            print(f\"  Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n    \n    return total_loss / len(dataloader)\n\n\n# Updated evaluation function that handles list captions\ndef evaluate(model, dataloader, criterion, device, pad_idx):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch[\"image\"].to(device)\n            captions = batch[\"caption\"]\n            \n            # Skip if captions is a list (can't calculate loss on multiple captions)\n            if isinstance(captions, list):\n                continue\n            \n            captions = captions.to(device)\n            caption_input = captions[:, :-1]\n            caption_target = captions[:, 1:]\n            \n            tgt_mask, tgt_padding_mask = create_mask(images, caption_input, pad_idx)\n            output = model(images, caption_input, tgt_mask, tgt_padding_mask)\n            \n            output = output.reshape(-1, output.shape[-1])\n            caption_target = caption_target.reshape(-1)\n            loss = criterion(output, caption_target)\n            \n            total_loss += loss.item()\n            num_batches += 1\n    \n    return total_loss / num_batches if num_batches > 0 else float('inf')\n\n# Now continue training from where it stopped\nprint(\"Resuming training...\")\nprint(\"=\"*70)\n\nfor epoch in range(1, num_epochs):  # Continue remaining epochs\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 70)\n    \n    # Train\n    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device, pad_idx)\n    train_losses.append(train_loss)\n    \n    # Evaluate\n    val_loss = evaluate(model, eval_dataloader, criterion, device, pad_idx)\n    val_losses.append(val_loss)\n    \n    scheduler.step()\n    \n    print(f\"Epoch {epoch + 1} Summary:\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"best_caption_model.pth\")\n        print(f\"  ✓ New best model saved!\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Training completed!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T09:54:51.771593Z","iopub.execute_input":"2026-02-19T09:54:51.772675Z","iopub.status.idle":"2026-02-19T09:57:00.830033Z","shell.execute_reply.started":"2026-02-19T09:54:51.772612Z","shell.execute_reply":"2026-02-19T09:57:00.828713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 3  \nCHECKPOINT_PATH = 'full_checkpoint.pth'\nstart_epoch = 0\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\n\nif os.path.exists(CHECKPOINT_PATH):\n    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    train_losses = checkpoint['train_losses']\n    val_losses = checkpoint['val_losses']\n    best_val_loss = checkpoint['best_val_loss']\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resumed from epoch {start_epoch}, best val loss: {best_val_loss:.4f}\")\n\n# 3. Run training (this will take ~2-3 hours total)\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 70)\n    \n    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device, pad_idx)\n    train_losses.append(train_loss)\n    \n    val_loss = evaluate(model, eval_dataloader, criterion, device, pad_idx)\n    val_losses.append(val_loss)\n    \n    scheduler.step()\n    \n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"/kaggle/working/best_caption_model.pth\")\n        print(\"Best model saved \")\n\nprint(\"\\nTraining complete \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T04:54:44.181527Z","iopub.execute_input":"2026-02-25T04:54:44.182132Z","iopub.status.idle":"2026-02-25T05:18:26.466937Z","shell.execute_reply.started":"2026-02-25T04:54:44.182103Z","shell.execute_reply":"2026-02-25T05:18:26.466249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the final best model with correct epoch number\ntorch.save({\n    'epoch': 2,  # 0-indexed, so epoch 3 = index 2\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n    'best_val_loss': best_val_loss,\n}, '/kaggle/working/full_checkpoint.pth')\n\n# Save tokenizer\nwith open('/kaggle/working/tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n\nprint(\"Done! Files saved:\")\nprint(f\"  Val losses: {val_losses}\")\nprint(f\"  Best val loss: {best_val_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:21:01.800567Z","iopub.execute_input":"2026-02-25T05:21:01.801335Z","iopub.status.idle":"2026-02-25T05:21:03.078621Z","shell.execute_reply.started":"2026-02-25T05:21:01.801304Z","shell.execute_reply":"2026-02-25T05:21:03.077877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save everything RIGHT NOW while it's still in memory\ntorch.save({\n    'epoch': 1,  # it was on epoch 2 (index 1) when interrupted\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n    'best_val_loss': best_val_loss,\n}, 'full_checkpoint.pth')\nprint(\"Emergency checkpoint saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:21:06.845509Z","iopub.execute_input":"2026-02-25T05:21:06.846084Z","iopub.status.idle":"2026-02-25T05:21:08.579214Z","shell.execute_reply.started":"2026-02-25T05:21:06.846050Z","shell.execute_reply":"2026-02-25T05:21:08.578387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD THE PRETRAINED MODEL","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/best_caption_model.pth\"))\nmodel.eval()\nprint(\"Best model loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:21:56.956552Z","iopub.execute_input":"2026-02-25T05:21:56.957270Z","iopub.status.idle":"2026-02-25T05:21:57.276517Z","shell.execute_reply.started":"2026-02-25T05:21:56.957233Z","shell.execute_reply":"2026-02-25T05:21:57.275891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EVALUATE THE MODEL","metadata":{}},{"cell_type":"code","source":"def generate_caption(model, image, tokenizer, max_len=50, device=\"cpu\"):\n    model.eval()\n    \n    with torch.no_grad():\n        # Process image\n        image = image.unsqueeze(0).to(device)\n        \n        # Start with start token\n        caption = [tokenizer.word2idx[\"<|startoftext|>\"]]\n        \n        for _ in range(max_len):\n            caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n            \n            tgt_mask = generate_square_subsequent_mask(len(caption), device=device)\n            \n            output = model(image, caption_tensor, tgt_mask=tgt_mask)\n            \n            # Get the last token prediction\n            next_token_logits = output[0, -1, :]\n            next_token = next_token_logits.argmax().item()\n            \n            caption.append(next_token)\n            \n            # Stop if end token is generated\n            if next_token == tokenizer.word2idx[\"<|endoftext|>\"]:\n                break\n        \n        # Decode caption\n        generated_caption = tokenizer.decode(caption)\n        \n        # Clean up the caption\n        generated_caption = generated_caption.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n        \n        return generated_caption\n\n\n# Evaluate on test set\nprint(\"\\nEvaluating on test set...\")\nprint(\"=\"*70)\n\n# Select random test samples\nnum_samples = 5\ntest_indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n\nfor idx in test_indices:\n    sample = test_dataset[idx]\n    image = sample[\"image\"]\n    actual_captions = sample[\"caption\"]\n    image_name = sample[\"image_name\"]\n    \n    # Generate caption\n    generated_caption = generate_caption(model, image, tokenizer, device=device)\n    \n    print(f\"\\nImage: {image_name}\")\n    print(f\"Generated: {generated_caption}\")\n    print(f\"Actual captions:\")\n    for i, cap in enumerate(actual_captions[:3], 1):\n        print(f\"  {i}. {cap}\")\n    print(\"-\" * 70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:21:59.344418Z","iopub.execute_input":"2026-02-25T05:21:59.345086Z","iopub.status.idle":"2026-02-25T05:22:02.334213Z","shell.execute_reply.started":"2026-02-25T05:21:59.345059Z","shell.execute_reply":"2026-02-25T05:22:02.333653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEPLOY THE MODEL USING GRADIO","metadata":{}},{"cell_type":"code","source":"# Create Gradio interface\ndef predict_caption(image):\n    \"\"\"Generate caption for uploaded image\"\"\"\n    # Transform image\n    image_tensor = tfms(image)\n    \n    # Generate caption\n    caption = generate_caption(model, image_tensor, tokenizer, device=device)\n    \n    return caption\n\n\n# Create Gradio interface\ndemo = gr.Interface(\n    fn=predict_caption,\n    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n    outputs=gr.Textbox(label=\"Generated Caption\"),\n    title=\"Image Caption Generator\",\n    description=\"Upload an image and get an AI-generated caption!\",\n    examples=[\n        [\"/kaggle/input/datasets/adityajn105/flickr8k/Images/\" + test_dataset.images[0]],\n        [\"/kaggle/input/datasets/adityajn105/flickr8k/Images/\" + test_dataset.images[1]],\n        [\"/kaggle/input/datasets/adityajn105/flickr8k/Images/\" + test_dataset.images[2]],\n    ]\n)\n\n# Launch the interface\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:22:04.875514Z","iopub.execute_input":"2026-02-25T05:22:04.876217Z","iopub.status.idle":"2026-02-25T05:22:05.961409Z","shell.execute_reply.started":"2026-02-25T05:22:04.876190Z","shell.execute_reply":"2026-02-25T05:22:05.960878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# List everything in /kaggle/working recursively\nprint(\"Contents of /kaggle/working:\")\nfor root, dirs, files in os.walk('/kaggle/working'):\n    level = root.replace('/kaggle/working', '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f'{indent}{os.path.basename(root)}/')\n    subindent = ' ' * 2 * (level + 1)\n    for file in files:\n        print(f'{subindent}{file}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:43:11.871669Z","iopub.execute_input":"2026-02-25T05:43:11.872247Z","iopub.status.idle":"2026-02-25T05:43:11.878837Z","shell.execute_reply.started":"2026-02-25T05:43:11.872216Z","shell.execute_reply":"2026-02-25T05:43:11.877891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import base64\nimport pickle\n\n# Convert tokenizer to base64 string and print it\nwith open('/kaggle/working/tokenizer.pkl', 'rb') as f:\n    data = f.read()\n    b64 = base64.b64encode(data).decode('utf-8')\n    print(\"TOKENIZER_START\")\n    print(b64)\n    print(\"TOKENIZER_END\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T05:53:22.609422Z","iopub.execute_input":"2026-02-25T05:53:22.610313Z","iopub.status.idle":"2026-02-25T05:53:22.618391Z","shell.execute_reply.started":"2026-02-25T05:53:22.610282Z","shell.execute_reply":"2026-02-25T05:53:22.617629Z"}},"outputs":[],"execution_count":null}]}